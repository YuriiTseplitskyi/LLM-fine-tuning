{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support)\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,        \n",
    "    DistilBertForSequenceClassification,  \n",
    "    Trainer,                     \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from peft import get_peft_model, PromptTuningConfig, TaskType\n",
    "\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MAX_LENGTH = 128\n",
    "NUM_LABELS = 3\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "main_path = '' # Path to main directory\n",
    "train_path = f'{main_path}data\\\\cleaned\\\\train.csv'\n",
    "test_path = f'{main_path}data\\\\cleaned\\\\test.csv'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc669a468299e2f6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a091329defa1c5da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8ad223d8939e2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(train_path: str, test_path: str) -> tuple[Dataset, Dataset]:\n",
    "\n",
    "    train_df, test_df = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "    return Dataset.from_pandas(train_df), Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21d3bd652fdb74",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_prompt(examples, prompt: str)->dict:\n",
    "    examples['post'] = [prompt + post for post in examples['post']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e98d145314f390",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_data(tokenizer, dataset: Dataset, prompt: str = None) -> Dataset:\n",
    "    \n",
    "    if prompt:\n",
    "        dataset = dataset.map(lambda examples: add_prompt(examples, prompt), batched=True)\n",
    "    \n",
    "    def tokenize(examples):\n",
    "        return tokenizer(examples['post'], padding='max_length', truncation=True, max_length=128)\n",
    "    \n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7c57b019a83bf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_datasets(train_path: str, \n",
    "                 test_path: str, \n",
    "                 tokenizer, \n",
    "                 prompt: str = None)->(Dataset, Dataset):\n",
    "    \n",
    "    train_dataset, val_dataset = load_data(train_path, test_path)\n",
    "    train_dataset = tokenize_data(tokenizer, train_dataset, prompt)\n",
    "    val_dataset = tokenize_data(tokenizer, val_dataset, prompt)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2edb0463c2faa3c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def freeze_all_layers(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3649f018ae6054dd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44ee7816cb42c8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unfreeze_specific_layers(model, parameters_to_unfreeze: list = None):\n",
    "            \n",
    "    if parameters_to_unfreeze is None:\n",
    "        parameters_to_unfreeze = ['classifier.bias', \n",
    "                                  'classifier.weight', \n",
    "                                  'pre_classifier.bias',\n",
    "                                  'pre_classifier.weight']\n",
    "        \n",
    "    for name, param in model.named_parameters():\n",
    "        if any([name.startswith(param_name) for param_name in parameters_to_unfreeze]):\n",
    "            param.requires_grad = True\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0e070f823ebf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model(freeze_all: bool = False, unfreeze_specific: bool = False, parameters_to_unfreeze: list = None):\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    if freeze_all:\n",
    "        model = freeze_all_layers(model)\n",
    "    \n",
    "    if unfreeze_specific:\n",
    "        model = unfreeze_specific_layers(model, parameters_to_unfreeze)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf466c92ae0be5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def compute_metrics(pred)->dict:\n",
    "    \n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, \n",
    "                                                               preds, \n",
    "                                                               average='weighted', \n",
    "                                                               zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, \n",
    "            'f1': f1, \n",
    "            'precision': precision, \n",
    "            'recall': recall}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a34dbd89e42cdc34",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_metrics(metrics: dict):\n",
    "    print(f\"Accuracy: {metrics['eval_accuracy']:.4f}\")\n",
    "    print(f\"F1: {metrics['eval_f1']:.4f}\")\n",
    "    print(f\"Precision: {metrics['eval_precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['eval_recall']:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed62249ef2b0619a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad4be03bc9eae9cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layer Unfreezing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af757cab87658d8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1bc4a3b5565291",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fine_tune_with_layer_unfreezing():\n",
    "    \n",
    "    model, tokenizer = get_model(freeze_all=True, unfreeze_specific=True)\n",
    "    \n",
    "    train_dataset, val_dataset = get_datasets(train_path, test_path, tokenizer)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'{main_path}results\\\\unfrozen_layers',\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        num_train_epochs=10,\n",
    "        logging_dir=f'{main_path}logs\\\\unfrozen_layers',\n",
    "        logging_steps=400,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        seed=42,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning model...\")\n",
    "    tr_metrics=trainer.train()\n",
    "    print(\"Training metrics:\")\n",
    "    print(tr_metrics)\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    print(\"Evaluation results:\")\n",
    "    print_metrics(eval_results)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9b36758fcc99553"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4234a0eb23967e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, trial, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.trial = trial\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "\n",
    "        loss = super().training_step(model, inputs)\n",
    "\n",
    "        self.trial.report(loss.item(), step=self.state.global_step)\n",
    "\n",
    "        if self.trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56028d5b35c79888",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search():\n",
    "    \n",
    "    model, tokenizer = get_model(freeze_all=True, unfreeze_specific=True)\n",
    "    \n",
    "    train_dataset, val_dataset = get_datasets(train_path, test_path, tokenizer)\n",
    "    \n",
    "    def objective(trial):\n",
    "        num_train_epochs = trial.suggest_int('num_train_epochs', 2, 15)\n",
    "        per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [8, 16, 32, 64, 128])\n",
    "        learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 0.1)\n",
    "        warmup_steps = trial.suggest_int('warmup_steps', 0, 500)\n",
    "        weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'{main_path}results\\\\hyperparam_tuning',\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=64,\n",
    "            logging_dir=f'{main_path}logs\\\\hyperparam_tuning',\n",
    "            logging_steps=400,\n",
    "            learning_rate=learning_rate,\n",
    "            lr_scheduler_type='linear',\n",
    "            warmup_steps=warmup_steps,\n",
    "            weight_decay=weight_decay,\n",
    "            eval_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            seed=42,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='accuracy'\n",
    "        )\n",
    "\n",
    "        trainer = CustomTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            trial=trial\n",
    "        )\n",
    "    \n",
    "        \n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        return eval_results['eval_accuracy']\n",
    "\n",
    "    study = optuna.create_study(study_name='hyperparam_tuning',\n",
    "                                sampler=optuna.samplers.TPESampler(seed=42),\n",
    "                                direction='maximize',\n",
    "                                pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5, interval_steps=1))\n",
    "    \n",
    "    study.optimize(objective, n_trials=40)\n",
    "    best_params = study.best_params\n",
    "    \n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(best_params)\n",
    "    \n",
    "    return best_params, model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13eb06e8983c2fa0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105829de70bbfdbb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prompt_tuning(best_params: dict, prompts: list):\n",
    "    \n",
    "    all_metrics = {}\n",
    "    best_model = None\n",
    "    best_tokenizer = None\n",
    "    max_accuracy = 0\n",
    "    best_prompt = None\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"Using prompt: {prompt}\")\n",
    "        \n",
    "        model, tokenizer = get_model(freeze_all=True)\n",
    "        \n",
    "        train_dataset, val_dataset = get_datasets(train_path, test_path, tokenizer, prompt)\n",
    "        \n",
    "        config = PromptTuningConfig(\n",
    "            peft_type='PROMPT_TUNING',\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            num_virtual_tokens=15, \n",
    "            num_transformer_submodules=1,\n",
    "            num_attention_heads=model.config.num_attention_heads,\n",
    "            num_layers=model.config.num_hidden_layers,\n",
    "            token_dim=model.config.dim,\n",
    "            prompt_tuning_init='TEXT',\n",
    "            prompt_tuning_init_text=\"Predict if sentiment of this review is positive, negative, or neutral.\",\n",
    "            tokenizer_name_or_path=MODEL_NAME\n",
    "        )\n",
    "        \n",
    "        peft_model = get_peft_model(model, config)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f'{main_path}results\\\\prompt_tuning_{prompts.index(prompt)}',\n",
    "            num_train_epochs=best_params['num_train_epochs'],\n",
    "            per_device_train_batch_size=best_params['per_device_train_batch_size'],\n",
    "            per_device_eval_batch_size=64,\n",
    "            logging_dir= f'{main_path}logs\\\\prompt_tuning_{prompts.index(prompt)}',\n",
    "            logging_steps=400,\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            lr_scheduler_type='linear',\n",
    "            warmup_steps=best_params['warmup_steps'],\n",
    "            weight_decay=best_params['weight_decay'],\n",
    "            eval_strategy='epoch',\n",
    "            save_strategy='epoch',\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=peft_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        print(f\"Starting prompt tuning with current prompt: {prompts.index(prompt)}\")\n",
    "        tr_metrics = trainer.train()\n",
    "        print(\"Training metrics:\")\n",
    "        print(tr_metrics)\n",
    "        \n",
    "        print(\"Evaluating model...\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(\"Evaluation results:\")\n",
    "        print_metrics(eval_results)\n",
    "\n",
    "        all_metrics[prompt] = eval_results\n",
    "        \n",
    "        if eval_results['eval_accuracy'] > max_accuracy:\n",
    "            max_accuracy = eval_results['eval_accuracy']\n",
    "            best_model = peft_model\n",
    "            best_tokenizer = tokenizer\n",
    "            best_prompt = prompt\n",
    "            \n",
    "\n",
    "    print(\"\\nSummary of evaluation metrics for all prompts:\")\n",
    "    for prompt, metrics in all_metrics.items():\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        print_metrics(metrics)\n",
    "    \n",
    "    return all_metrics, best_model, best_tokenizer, best_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddd1b0ac1393845d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Layer Unfreezing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86fdffaf6e07cf27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b0dedf8dcd4ec",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model, tokenizer = fine_tune_with_layer_unfreezing()\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "model.save_pretrained(f'{main_path}models/unfrozen_layers')\n",
    "tokenizer.save_pretrained(f'{main_path}models/unfrozen_layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d287d25cda8fd61d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83487b463250e7c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_params, model, tokenizer = hyperparameter_search()\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "model.save_pretrained(f'{main_path}models/hyperparam_tuning')\n",
    "tokenizer.save_pretrained(f'{main_path}models/hyperparam_tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompt Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f900a71cc3a22351"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "'''\n",
    "best_params = {'num_train_epochs': 8, \n",
    "               'per_device_train_batch_size': 16, \n",
    "               'learning_rate': 0.0006672367170464204, \n",
    "               'warmup_steps': 393, \n",
    "               'weight_decay': 6.290644294586145e-06}\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40d652201e20d337",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab1d4d75f721af",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The sentiment of this review is:\", \n",
    "    \"This tweet expresses a sentiment that is:\",\n",
    "    \"Sentiment classification of this message:\",\n",
    "]\n",
    "\n",
    "metrics, model, tokenizer, prompt = prompt_tuning(best_params, prompts)\n",
    "print(metrics)\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "model.save_pretrained(f'{main_path}models/prompt_tuning')\n",
    "tokenizer.save_pretrained(f'{main_path}models/prompt_tuning')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
